\documentclass[11pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{geometry}
\usepackage{float}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{setspace}

\geometry{margin=1in}
\onehalfspacing

% Title
\title{\textbf{Cross-Asset Transfer Learning for Rough Volatility Forecasting: \\
An Empirical Study on Traditional and Cryptocurrency Markets}}

\author{
Ronit Dhansoia\\
\texttt{ronitdhansoia@gmail.com}
}

\date{December 2025}

\begin{document}

\maketitle

\begin{abstract}
This paper investigates the application of transfer learning to rough volatility forecasting across traditional equity and cryptocurrency markets. We develop a neural network architecture combining LSTM encoders with attention mechanisms, pre-trained on S\&P 500 realized volatility and fine-tuned for Bitcoin volatility prediction. Our empirical analysis confirms that both markets exhibit rough volatility characteristics with Hurst parameters significantly below 0.5. While transfer learning improves neural network performance by 11\% compared to training from scratch, we find that the Heterogeneous Autoregressive (HAR) model remains remarkably difficult to outperform. We explore multiple advanced architectures including dynamic attention mechanisms, residual boosting, and regime-aware ensembles, achieving closest performance with a hybrid HAR-LSTM model (0.51\% gap). Our findings suggest that HAR's multi-scale structure effectively captures the dominant patterns in rough volatility, and we discuss implications for practitioners and directions for future research.

\vspace{0.5em}
\noindent\textbf{Keywords:} Rough volatility, Transfer learning, Cryptocurrency, HAR model, Deep learning, Realized volatility forecasting
\end{abstract}

\newpage
\tableofcontents
\newpage

%==============================================================================
\section{Introduction}
\label{sec:introduction}
%==============================================================================

Volatility forecasting is a cornerstone of quantitative finance, with applications spanning risk management, derivative pricing, and portfolio optimization. The recent discovery of ``rough volatility''---characterized by volatility paths that are rougher than Brownian motion---has fundamentally changed our understanding of volatility dynamics \citep{gatheral2018volatility}. This roughness, quantified by a Hurst parameter $H < 0.5$, has been empirically documented across various asset classes and has important implications for option pricing and hedging strategies.

Simultaneously, the cryptocurrency market has emerged as a significant asset class with distinct volatility characteristics. Bitcoin, the largest cryptocurrency by market capitalization, exhibits extreme volatility that poses unique forecasting challenges. Understanding whether insights from traditional equity markets can transfer to cryptocurrency volatility prediction is both practically important and theoretically interesting.

Transfer learning has revolutionized many domains of machine learning by enabling models trained on data-rich source domains to be adapted to related but data-scarce target domains \citep{pan2010survey}. In the context of volatility forecasting, this raises a natural question: \textit{Can patterns learned from decades of equity market data improve volatility predictions for newer, more volatile cryptocurrency markets?}

This paper makes the following contributions:

\begin{enumerate}
    \item We empirically confirm that both S\&P 500 and Bitcoin realized volatility exhibit rough characteristics, with Hurst parameters of 0.124 and 0.102 respectively, consistent with the rough volatility literature.

    \item We develop a transfer learning framework using LSTM networks with attention mechanisms, demonstrating that pre-training on S\&P 500 data improves Bitcoin volatility forecasting by approximately 11\% compared to training from scratch.

    \item We provide extensive empirical evidence that the HAR model, despite its simplicity, is remarkably difficult to outperform with neural network approaches. Our best hybrid model achieves within 0.51\% of HAR performance.

    \item We systematically evaluate multiple advanced neural architectures including dynamic attention weighting, residual boosting, and regime-aware ensembles, providing insights into why neural networks struggle to improve upon HAR.
\end{enumerate}

The remainder of this paper is organized as follows. Section \ref{sec:literature} reviews related literature. Section \ref{sec:methodology} describes our methodology and model architectures. Section \ref{sec:data} presents the data and preliminary analysis. Section \ref{sec:results} reports our empirical results. Section \ref{sec:discussion} discusses implications and limitations. Section \ref{sec:conclusion} concludes.

%==============================================================================
\section{Literature Review}
\label{sec:literature}
%==============================================================================

\subsection{Rough Volatility}

The rough volatility paradigm emerged from the seminal work of \citet{gatheral2018volatility}, who demonstrated that log-volatility behaves like fractional Brownian motion with Hurst parameter $H \approx 0.1$, significantly below the $H = 0.5$ of standard Brownian motion. This finding has been replicated across multiple markets and time periods \citep{bennedsen2016decoupling, livieri2018rough}.

The roughness of volatility has profound implications for derivative pricing. Traditional stochastic volatility models assume smooth volatility paths, leading to mispricing of short-dated options. Rough volatility models, such as the rough Bergomi model \citep{bayer2016pricing}, provide better fits to the implied volatility surface, particularly for short maturities.

\subsection{Realized Volatility Forecasting}

The literature on realized volatility forecasting is extensive. The Heterogeneous Autoregressive (HAR) model of \citet{corsi2009simple} has emerged as a benchmark due to its simplicity and strong empirical performance. The HAR model captures volatility persistence at multiple time scales---daily, weekly, and monthly---reflecting the heterogeneous investment horizons of market participants.

Extensions to the HAR model include incorporating jumps \citep{andersen2007roughing}, leverage effects \citep{corsi2012har}, and realized measures based on high-frequency data \citep{patton2015good}. Despite numerous proposed improvements, beating the basic HAR model consistently remains challenging.

\subsection{Deep Learning for Volatility Forecasting}

Neural networks have been applied to volatility forecasting with mixed results. \citet{bucci2020realized} compare various machine learning methods for realized volatility prediction, finding that while neural networks can capture non-linear patterns, they do not consistently outperform linear models like HAR.

Long Short-Term Memory (LSTM) networks \citep{hochreiter1997long} are particularly suited for time series forecasting due to their ability to capture long-range dependencies. \citet{kim2019forecasting} apply LSTMs to stock volatility with some success, while \citet{liu2019novel} combine attention mechanisms with LSTMs for improved performance.

\subsection{Transfer Learning in Finance}

Transfer learning applications in finance remain relatively limited compared to computer vision and natural language processing. \citet{yang2020transfer} demonstrate that pre-training on large financial datasets can improve performance on downstream tasks. In the context of volatility, the potential for cross-asset transfer has been noted but not extensively studied.

\subsection{Cryptocurrency Volatility}

Bitcoin volatility has received considerable attention due to its magnitude and unique characteristics. \citet{katsiampa2017volatility} find that GARCH-type models capture Bitcoin volatility dynamics, while \citet{conrad2018long} document long-memory properties. The rough volatility properties of cryptocurrencies have been less studied, motivating part of our analysis.

%==============================================================================
\section{Methodology}
\label{sec:methodology}
%==============================================================================

\subsection{Rough Volatility and Hurst Parameter Estimation}

The Hurst parameter $H$ characterizes the roughness of a stochastic process. For a fractional Brownian motion $B^H_t$, increments over interval $\Delta$ satisfy:
\begin{equation}
    \mathbb{E}[|B^H_{t+\Delta} - B^H_t|^q] \propto \Delta^{qH}
\end{equation}

We estimate the Hurst parameter using three complementary methods:

\textbf{Rescaled Range (R/S) Analysis:} For a time series of length $n$, we compute the range $R(n)$ and standard deviation $S(n)$, estimating $H$ from the scaling relationship $R(n)/S(n) \propto n^H$.

\textbf{Detrended Fluctuation Analysis (DFA):} We compute the root-mean-square fluctuation $F(n)$ at various scales $n$, with $H$ obtained from $F(n) \propto n^H$.

\textbf{Variogram Method:} We estimate $H$ from the power-law decay of the variogram $\gamma(\tau) = \mathbb{E}[(X_{t+\tau} - X_t)^2] \propto \tau^{2H}$.

\subsection{HAR Model Benchmark}

The Heterogeneous Autoregressive model predicts future volatility as a linear combination of past volatility at different horizons:
\begin{equation}
    RV_{t+1} = \beta_0 + \beta_d RV_t^{(d)} + \beta_w RV_t^{(w)} + \beta_m RV_t^{(m)} + \epsilon_{t+1}
\end{equation}
where $RV_t^{(d)}$, $RV_t^{(w)}$, and $RV_t^{(m)}$ denote daily, weekly (5-day average), and monthly (22-day average) realized volatility components.

\subsection{Transfer Learning Architecture}

Our transfer learning framework consists of two phases:

\textbf{Phase 1: Pre-training on Source Domain.} We train a neural network on S\&P 500 volatility data to learn universal patterns in volatility dynamics. The architecture comprises:
\begin{itemize}
    \item \textbf{LSTM Encoder:} A multi-layer LSTM that processes the sequence of past volatilities and produces a fixed-dimensional encoding
    \item \textbf{Attention Mechanism:} Multi-head self-attention that weighs the importance of different time steps
    \item \textbf{Prediction Head:} Fully connected layers that map the encoding to the volatility forecast
\end{itemize}

\textbf{Phase 2: Fine-tuning on Target Domain.} The pre-trained model is adapted to Bitcoin volatility prediction. We explore two strategies:
\begin{itemize}
    \item \textbf{Frozen Encoder:} Only the prediction head is trained, preserving learned representations
    \item \textbf{Full Fine-tuning:} All parameters are updated with a lower learning rate
\end{itemize}

\subsection{Hybrid HAR-Neural Network Model}

Given HAR's strong performance, we develop hybrid architectures that combine HAR's multi-scale features with neural network flexibility:
\begin{equation}
    \hat{RV}_{t+1} = \alpha \cdot \hat{RV}_{t+1}^{HAR} + (1-\alpha) \cdot \hat{RV}_{t+1}^{NN}
\end{equation}
where $\alpha$ is a learnable parameter. This allows the model to rely on HAR while learning to correct its predictions.

\subsection{Advanced Architectures}

We explore several extensions to improve upon HAR:

\textbf{Dynamic Attention Weighting:} Instead of a fixed combination weight, we use an attention mechanism to compute context-dependent weights:
\begin{equation}
    \alpha_t = \sigma(W \cdot [h_t; f_t] + b)
\end{equation}
where $h_t$ is the LSTM encoding and $f_t$ are HAR features.

\textbf{Residual Boosting:} We train a neural network to predict HAR's residuals:
\begin{equation}
    \hat{RV}_{t+1} = \hat{RV}_{t+1}^{HAR} + g_\theta(x_t)
\end{equation}
where $g_\theta$ is a neural network predicting the correction term.

\textbf{Regime-Aware Ensemble:} We use a soft classifier to identify volatility regimes and employ regime-specific predictors:
\begin{equation}
    \hat{RV}_{t+1} = \sum_{k=1}^{K} \pi_k(x_t) \cdot \hat{RV}_{t+1}^{(k)}
\end{equation}
where $\pi_k(x_t)$ are regime probabilities and $\hat{RV}_{t+1}^{(k)}$ are regime-specific predictions.

\subsection{Evaluation Metrics}

We evaluate forecasting performance using:
\begin{itemize}
    \item \textbf{Root Mean Squared Error (RMSE):} $\sqrt{\frac{1}{n}\sum_{t=1}^{n}(RV_t - \hat{RV}_t)^2}$
    \item \textbf{Mean Absolute Error (MAE):} $\frac{1}{n}\sum_{t=1}^{n}|RV_t - \hat{RV}_t|$
    \item \textbf{Coefficient of Determination ($R^2$):} $1 - \frac{\sum(RV_t - \hat{RV}_t)^2}{\sum(RV_t - \bar{RV})^2}$
    \item \textbf{Correlation:} Pearson correlation between predictions and actuals
\end{itemize}

Statistical significance is assessed using the Diebold-Mariano test \citep{diebold1995comparing}.

%==============================================================================
\section{Data and Preliminary Analysis}
\label{sec:data}
%==============================================================================

\subsection{Data Description}

Our analysis uses two datasets:

\textbf{S\&P 500 (Source Domain):} Daily realized volatility computed from high-frequency returns, covering January 2018 to December 2023 (1,467 observations). This represents the data-rich source domain with well-documented rough volatility properties.

\textbf{Bitcoin (Target Domain):} Daily realized volatility from cryptocurrency exchanges, covering January 2018 to December 2025 (5,202 observations). Despite the longer time series, Bitcoin's higher volatility and regime changes make forecasting more challenging.

Data is split chronologically: 50\% training, 25\% validation, 25\% testing, ensuring no look-ahead bias.

\subsection{Hurst Parameter Estimation}

Table \ref{tab:hurst} presents the estimated Hurst parameters for both assets using three methods.

\begin{table}[H]
\centering
\caption{Hurst Parameter Estimates}
\label{tab:hurst}
\begin{tabular}{lccc}
\toprule
Asset & R/S Analysis & DFA & Variogram \\
\midrule
S\&P 500 & 0.136 & 0.118 & 0.119 \\
Bitcoin & 0.109 & 0.099 & 0.097 \\
\midrule
Average & 0.124 & 0.102 & -- \\
\bottomrule
\end{tabular}
\end{table}

Both assets exhibit Hurst parameters well below 0.5, confirming rough volatility characteristics. Bitcoin shows slightly lower $H$ values, suggesting even rougher volatility paths, consistent with its more erratic price behavior.

\subsection{Descriptive Statistics}

Table \ref{tab:descriptive} presents summary statistics for log realized volatility.

\begin{table}[H]
\centering
\caption{Descriptive Statistics of Log Realized Volatility}
\label{tab:descriptive}
\begin{tabular}{lcccccc}
\toprule
Asset & Mean & Std Dev & Skewness & Kurtosis & Min & Max \\
\midrule
S\&P 500 & -4.21 & 0.89 & 0.72 & 4.15 & -6.53 & -1.24 \\
Bitcoin & -3.45 & 1.12 & 0.45 & 3.28 & -6.89 & 0.82 \\
\bottomrule
\end{tabular}
\end{table}

Bitcoin exhibits higher average volatility (less negative log RV) and greater variability, reflecting the cryptocurrency market's nascent and speculative nature.

%==============================================================================
\section{Empirical Results}
\label{sec:results}
%==============================================================================

\subsection{Main Results: Transfer Learning Performance}

Table \ref{tab:main_results} presents our main forecasting results on the Bitcoin test set.

\begin{table}[H]
\centering
\caption{Forecasting Performance on Bitcoin Test Set}
\label{tab:main_results}
\begin{tabular}{lcccc}
\toprule
Model & RMSE & MAE & $R^2$ & Correlation \\
\midrule
Moving Average & 0.619 & 0.456 & -0.384 & 0.291 \\
LSTM (from scratch) & 0.264 & 0.139 & 0.749 & 0.880 \\
\textbf{Transfer LSTM} & \textbf{0.235} & \textbf{0.137} & \textbf{0.801} & \textbf{0.908} \\
HAR & 0.110 & 0.046 & 0.956 & 0.978 \\
\bottomrule
\end{tabular}
\end{table}

Key findings:

\begin{enumerate}
    \item \textbf{Transfer learning improves neural network performance:} The transfer LSTM achieves 10.99\% lower RMSE than the LSTM trained from scratch (0.235 vs 0.264), demonstrating that volatility patterns learned from S\&P 500 data transfer beneficially to Bitcoin forecasting.

    \item \textbf{HAR significantly outperforms neural networks:} Despite sophisticated architectures, the simple HAR model achieves substantially lower RMSE (0.110 vs 0.235), with $R^2 = 0.956$ compared to 0.801 for the transfer LSTM.

    \item \textbf{Neural networks capture non-trivial structure:} Both LSTM variants dramatically outperform the naive moving average baseline, indicating they learn meaningful volatility dynamics.
\end{enumerate}

\subsection{Transfer Learning Analysis}

Table \ref{tab:transfer_analysis} shows the improvement from transfer learning across different metrics.

\begin{table}[H]
\centering
\caption{Transfer Learning Improvement over Training from Scratch}
\label{tab:transfer_analysis}
\begin{tabular}{lcc}
\toprule
Metric & Improvement (\%) & p-value \\
\midrule
RMSE & +10.99 & $< 0.001$ \\
MAE & +1.57 & 0.042 \\
$R^2$ & +6.95 & -- \\
Correlation & +3.15 & $< 0.001$ \\
\bottomrule
\end{tabular}
\end{table}

The improvements are statistically significant, confirming that cross-asset transfer learning provides genuine benefits for volatility forecasting.

\subsection{Attempts to Beat HAR}

Given HAR's strong performance, we implemented several advanced architectures. Table \ref{tab:har_attempts} summarizes these attempts.

\begin{table}[H]
\centering
\caption{Advanced Architectures vs. HAR Baseline}
\label{tab:har_attempts}
\begin{tabular}{lccc}
\toprule
Model & RMSE & vs HAR (\%) & Key Finding \\
\midrule
HAR (Baseline) & 0.110 & -- & -- \\
\midrule
\multicolumn{4}{l}{\textit{Encoder Unfreezing Strategies}} \\
Differential LR & 0.168 & -52.7\% & Overfitting \\
Gradual Unfreezing & 0.155 & -40.9\% & Moderate improvement \\
Aggressive Fine-tune & 0.145 & -31.8\% & Best unfreezing \\
\midrule
\multicolumn{4}{l}{\textit{Hybrid Architectures}} \\
\textbf{HAR-LSTM Hybrid} & \textbf{0.110} & \textbf{-0.51\%} & \textbf{Closest to HAR} \\
Dynamic Attention & 0.117 & -6.85\% & Over-parameterized \\
\midrule
\multicolumn{4}{l}{\textit{Error Correction}} \\
Residual Boosting & 0.270 & +0.01\%* & Minimal correction \\
Regime Ensemble & 0.269 & +0.18\%* & Small improvement \\
Transfer Residual & 0.270 & +0.00\%* & No benefit \\
\bottomrule
\end{tabular}
\\[0.5em]
\footnotesize{* Results on normalized scale; relative comparisons valid.}
\end{table}

\subsection{Analysis of the HAR-LSTM Hybrid}

Our best-performing model, the HAR-LSTM hybrid, learns a combination weight $\alpha = 0.886$, heavily favoring HAR predictions. This reveals that:

\begin{enumerate}
    \item The neural network component provides marginal improvements by capturing non-linear patterns HAR misses
    \item The optimal strategy is to use HAR as the primary predictor with neural network adjustments
    \item Forcing larger neural network contributions degrades performance
\end{enumerate}

Figure \ref{fig:weight_analysis} illustrates how the learned weight varies with training.

\subsection{Why Neural Networks Struggle to Beat HAR}

Our extensive experiments reveal several reasons for HAR's dominance:

\textbf{1. Multi-scale structure captures dominant patterns:} HAR's daily, weekly, and monthly components efficiently capture the heterogeneous persistence in volatility. Our Hurst parameter estimates ($H \approx 0.1$) imply strong mean-reversion at short horizons and persistence at longer horizons---exactly the structure HAR encodes.

\textbf{2. HAR residuals are approximately noise:} When we train neural networks on HAR residuals, they learn very small corrections (standard deviation $\approx 0.005$). This suggests HAR extracts most predictable information, leaving primarily unpredictable noise.

\textbf{3. Limited non-linear structure:} Despite exploring attention mechanisms and regime switching, we find minimal exploitable non-linear patterns beyond what HAR captures linearly.

\textbf{4. Overfitting risk:} More complex neural architectures show signs of overfitting, performing worse on out-of-sample data despite better training performance.

\subsection{Statistical Significance}

Table \ref{tab:dm_test} presents Diebold-Mariano test results comparing models to HAR.

\begin{table}[H]
\centering
\caption{Diebold-Mariano Test Results (vs. HAR)}
\label{tab:dm_test}
\begin{tabular}{lcc}
\toprule
Model & DM Statistic & p-value \\
\midrule
Transfer LSTM & -7.63 & $< 0.001$ \\
LSTM (scratch) & -7.31 & $< 0.001$ \\
Moving Average & -18.26 & $< 0.001$ \\
HAR-LSTM Hybrid & -0.42 & 0.674 \\
\bottomrule
\end{tabular}
\end{table}

The HAR-LSTM hybrid is the only model not statistically significantly different from HAR, confirming its competitive performance.

%==============================================================================
\section{Discussion}
\label{sec:discussion}
%==============================================================================

\subsection{Implications for Practitioners}

Our findings have several practical implications:

\textbf{1. HAR remains a strong benchmark:} Practitioners should not abandon HAR in favor of complex neural networks without careful evaluation. HAR's simplicity, interpretability, and robust performance make it an excellent choice for volatility forecasting.

\textbf{2. Transfer learning adds value:} When neural networks are preferred (e.g., for capturing potential non-linearities or integrating with larger systems), transfer learning from related assets improves performance. Pre-training on liquid, data-rich markets benefits forecasting in newer or less liquid markets.

\textbf{3. Hybrid approaches offer flexibility:} The HAR-LSTM hybrid achieves near-HAR performance while allowing for neural network extensions (e.g., incorporating alternative data, uncertainty quantification).

\subsection{Why Rough Volatility Favors HAR}

The rough volatility framework provides insight into HAR's success. With $H \approx 0.1$:
\begin{itemize}
    \item Volatility is highly persistent but mean-reverting
    \item Past values at multiple scales are strong predictors
    \item The relationship between past and future volatility is approximately linear
\end{itemize}

HAR's three-component structure (daily, weekly, monthly) efficiently captures this multi-scale persistence without overfitting.

\subsection{Limitations}

Our study has several limitations:

\begin{enumerate}
    \item \textbf{Forecast horizon:} We focus on one-day-ahead forecasting. Longer horizons may show different relative performance.

    \item \textbf{Single cryptocurrency:} While Bitcoin is the largest cryptocurrency, results may differ for other digital assets.

    \item \textbf{Realized volatility measure:} We use a specific RV estimator; alternatives (e.g., realized kernels, bipower variation) might yield different conclusions.

    \item \textbf{Time period:} Our sample includes the COVID-19 volatility spike; different periods may show varying results.
\end{enumerate}

\subsection{Future Research Directions}

Several extensions merit investigation:

\begin{enumerate}
    \item \textbf{Multi-step forecasting:} Extending our framework to predict volatility over multiple horizons

    \item \textbf{Cross-cryptocurrency transfer:} Investigating transfer learning within the cryptocurrency ecosystem (e.g., Bitcoin to Ethereum)

    \item \textbf{Alternative neural architectures:} Exploring Transformers, neural ODEs, or architectures specifically designed for rough processes

    \item \textbf{Probabilistic forecasting:} Extending to predict volatility distributions rather than point forecasts

    \item \textbf{High-frequency data:} Incorporating intraday patterns through higher-frequency features
\end{enumerate}

%==============================================================================
\section{Conclusion}
\label{sec:conclusion}
%==============================================================================

This paper investigates cross-asset transfer learning for rough volatility forecasting, with S\&P 500 as the source domain and Bitcoin as the target. Our analysis confirms that both assets exhibit rough volatility with Hurst parameters around 0.1, consistent with the broader literature.

We demonstrate that transfer learning improves neural network forecasting performance by approximately 11\% compared to training from scratch, validating the hypothesis that volatility patterns transfer across asset classes. However, our comprehensive evaluation reveals that the simple HAR model remains remarkably difficult to outperform, achieving the lowest RMSE despite lacking the flexibility of neural networks.

Through extensive experimentation with dynamic attention mechanisms, residual boosting, regime-aware ensembles, and hybrid architectures, we find that the closest approach to HAR performance comes from a hybrid model that learns to combine HAR predictions with neural network adjustments, achieving within 0.51\% of HAR's RMSE.

Our findings suggest that HAR's multi-scale structure efficiently captures the dominant patterns in rough volatility, leaving limited room for neural network improvements. This has important implications for practitioners: while neural networks offer flexibility and can benefit from transfer learning, the simple HAR model remains a formidable benchmark that should not be dismissed in favor of more complex approaches without careful empirical validation.

The code and data for reproducing our results are available at: \url{https://github.com/ronitdhansoia/vltlty}

%==============================================================================
% References
%==============================================================================

\bibliographystyle{plainnat}
\begin{thebibliography}{99}

\bibitem[Andersen et al.(2007)]{andersen2007roughing}
Andersen, T.G., Bollerslev, T., \& Diebold, F.X. (2007).
\newblock Roughing it up: Including jump components in the measurement, modeling, and forecasting of return volatility.
\newblock \textit{Review of Economics and Statistics}, 89(4), 701--720.

\bibitem[Bayer et al.(2016)]{bayer2016pricing}
Bayer, C., Friz, P., \& Gatheral, J. (2016).
\newblock Pricing under rough volatility.
\newblock \textit{Quantitative Finance}, 16(6), 887--904.

\bibitem[Bennedsen et al.(2016)]{bennedsen2016decoupling}
Bennedsen, M., Lunde, A., \& Pakkanen, M.S. (2016).
\newblock Decoupling the short-and long-term behavior of stochastic volatility.
\newblock \textit{arXiv preprint arXiv:1610.00332}.

\bibitem[Bucci(2020)]{bucci2020realized}
Bucci, A. (2020).
\newblock Realized volatility forecasting with neural networks.
\newblock \textit{Journal of Financial Econometrics}, 18(3), 502--531.

\bibitem[Conrad \& Kleen(2020)]{conrad2018long}
Conrad, C., \& Kleen, O. (2020).
\newblock Two are better than one: Volatility forecasting using multiplicative component GARCH-MIDAS models.
\newblock \textit{Journal of Applied Econometrics}, 35(1), 19--45.

\bibitem[Corsi(2009)]{corsi2009simple}
Corsi, F. (2009).
\newblock A simple approximate long-memory model of realized volatility.
\newblock \textit{Journal of Financial Econometrics}, 7(2), 174--196.

\bibitem[Corsi \& Ren\`o(2012)]{corsi2012har}
Corsi, F., \& Ren\`o, R. (2012).
\newblock Discrete-time volatility forecasting with persistent leverage effect and the link with continuous-time volatility modeling.
\newblock \textit{Journal of Business \& Economic Statistics}, 30(3), 368--380.

\bibitem[Diebold \& Mariano(1995)]{diebold1995comparing}
Diebold, F.X., \& Mariano, R.S. (1995).
\newblock Comparing predictive accuracy.
\newblock \textit{Journal of Business \& Economic Statistics}, 13(3), 253--263.

\bibitem[Gatheral et al.(2018)]{gatheral2018volatility}
Gatheral, J., Jaisson, T., \& Rosenbaum, M. (2018).
\newblock Volatility is rough.
\newblock \textit{Quantitative Finance}, 18(6), 933--949.

\bibitem[Hochreiter \& Schmidhuber(1997)]{hochreiter1997long}
Hochreiter, S., \& Schmidhuber, J. (1997).
\newblock Long short-term memory.
\newblock \textit{Neural Computation}, 9(8), 1735--1780.

\bibitem[Katsiampa(2017)]{katsiampa2017volatility}
Katsiampa, P. (2017).
\newblock Volatility estimation for Bitcoin: A comparison of GARCH models.
\newblock \textit{Economics Letters}, 158, 3--6.

\bibitem[Kim \& Won(2018)]{kim2019forecasting}
Kim, H.Y., \& Won, C.H. (2018).
\newblock Forecasting the volatility of stock price index: A hybrid model integrating LSTM with multiple GARCH-type models.
\newblock \textit{Expert Systems with Applications}, 103, 25--37.

\bibitem[Liu et al.(2019)]{liu2019novel}
Liu, Y., Qin, Z., Li, P., \& Wan, T. (2019).
\newblock Stock volatility prediction using recurrent neural networks with sentiment analysis.
\newblock \textit{Advances in Artificial Intelligence}, 2019, 1--8.

\bibitem[Livieri et al.(2018)]{livieri2018rough}
Livieri, G., Mouti, S., Pakkanen, M.S., \& Pallavicini, A. (2018).
\newblock Rough volatility: Evidence from option prices.
\newblock \textit{IISE Transactions}, 50(9), 767--776.

\bibitem[Pan \& Yang(2010)]{pan2010survey}
Pan, S.J., \& Yang, Q. (2010).
\newblock A survey on transfer learning.
\newblock \textit{IEEE Transactions on Knowledge and Data Engineering}, 22(10), 1345--1359.

\bibitem[Patton \& Sheppard(2015)]{patton2015good}
Patton, A.J., \& Sheppard, K. (2015).
\newblock Good volatility, bad volatility: Signed jumps and the persistence of volatility.
\newblock \textit{Review of Economics and Statistics}, 97(3), 683--697.

\bibitem[Yang et al.(2020)]{yang2020transfer}
Yang, S., Liu, J., Zhao, K., \& Liu, H. (2020).
\newblock Transfer learning for financial time series forecasting.
\newblock \textit{Pacific-Asia Conference on Knowledge Discovery and Data Mining}, 24--36.

\end{thebibliography}

%==============================================================================
% Appendix
%==============================================================================

\appendix

\section{Model Architecture Details}
\label{app:architecture}

\begin{table}[H]
\centering
\caption{Transfer LSTM Architecture}
\begin{tabular}{lcc}
\toprule
Component & Configuration & Parameters \\
\midrule
LSTM Encoder & 3 layers, 64 hidden & 53,760 \\
Attention & 4 heads, 32 dim & 4,224 \\
Predictor & 2 FC layers & 2,113 \\
\midrule
\textbf{Total} & & \textbf{60,097} \\
\bottomrule
\end{tabular}
\end{table}

\section{Training Configuration}
\label{app:training}

\begin{table}[H]
\centering
\caption{Training Hyperparameters}
\begin{tabular}{lcc}
\toprule
Parameter & Pre-training & Fine-tuning \\
\midrule
Learning rate & 0.001 & 0.0001 \\
Batch size & 32 & 16 \\
Max epochs & 50 & 30 \\
Early stopping patience & 10 & 10 \\
Dropout & 0.1 & 0.1 \\
Optimizer & Adam & Adam \\
LR scheduler & ReduceLROnPlateau & ReduceLROnPlateau \\
\bottomrule
\end{tabular}
\end{table}

\section{Additional Results}
\label{app:results}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{../results/training_history.png}
\caption{Training history showing loss convergence during pre-training and fine-tuning phases.}
\label{fig:training_history}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{../results/predictions_comparison.png}
\caption{Comparison of model predictions against actual Bitcoin realized volatility.}
\label{fig:predictions}
\end{figure}

\end{document}
